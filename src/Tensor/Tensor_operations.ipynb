{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01e149bc",
   "metadata": {},
   "source": [
    "# Several Operations of Tensor in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae41aae6",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f875f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97aa59e3",
   "metadata": {},
   "source": [
    "## Creating Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19417273",
   "metadata": {},
   "source": [
    "- `zeros` and `ones` and `zeros_like`\n",
    "\n",
    "- `eyes` (Identity Matrix)\n",
    "\n",
    "- `rand` and `randm` matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb431cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For creating random Tensors\n",
    "\n",
    "# Create a randomized tensor between 0 and 1.\n",
    "random_tensor_1 = torch.rand(2, 3, 4)\n",
    "random_tensor_2 = torch.rand(2, 3, 4, dtype=torch.float16)\n",
    "print(random_tensor_1)\n",
    "print(random_tensor_2)\n",
    "\n",
    "# Using rand_like to keep the same size\n",
    "random_tensor_3 = torch.rand_like(random_tensor_2)\n",
    "print(random_tensor_3)\n",
    "\n",
    "# Random tensor with a uniform distribution of integers.\n",
    "random_tensor_4 = torch.randint(1, 100, (3, 4, 4))\n",
    "random_tensor_5 = torch.randint_like(random_tensor_4, high=10, low=1)\n",
    "print(random_tensor_4)\n",
    "print(random_tensor_5)\n",
    "\n",
    "# Random tensor with a normal distribution.\n",
    "random_tensor_6 = torch.randn(2, 3)\n",
    "print(random_tensor_6)\n",
    "random_tensor_7 = torch.randn(10000000)\n",
    "print(random_tensor_7)\n",
    "print(f\"Average: {random_tensor_7.mean():.4f}\")\n",
    "print(f\"Standard deviation: {random_tensor_7.std():.4f}\")\n",
    "\n",
    "# customizing mean and std, or using rand_like\n",
    "mean, std = 5.0, 2.0\n",
    "random_tensor_8 = mean + std * random_tensor_6\n",
    "random_tensor_9 = torch.randn_like(random_tensor_3)\n",
    "print(random_tensor_9)\n",
    "\n",
    "# Returns a tensor of random numbers drawn from separate normal distributions, independently sample each element.\n",
    "means = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "std = torch.tensor([[1, 2], [4, 5]])\n",
    "custom_normal = torch.normal(means, std)\n",
    "print(custom_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a473092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data shuffle\n",
    "perm = torch.randperm(5)\n",
    "print(perm)\n",
    "data = torch.tensor([10, 20, 30, 40, 50])\n",
    "shuffled = data[torch.randperm(len(data))]\n",
    "print(shuffled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2685d3",
   "metadata": {},
   "source": [
    "Well, you can also sample from different distributions...\n",
    "\n",
    "- `torch.bernoulli()`\n",
    "\n",
    "- `torch.poisson()`\n",
    "\n",
    "- `torch.multinomial()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25b56f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Normal, Uniform, Gamma\n",
    "\n",
    "# Normal Distribution\n",
    "normal_dist = Normal(loc=0.0, scale=1.0)\n",
    "samples = normal_dist.sample((2, 3))\n",
    "print(samples)\n",
    "\n",
    "# Uniform Distribution\n",
    "uniform_dist = Uniform(low=-1.0, high=1.0)\n",
    "samples = uniform_dist.sample((2, 3))\n",
    "print(samples)\n",
    "\n",
    "# Gamma Distribution\n",
    "gamma_dist = Gamma(concentration=1.0, rate=1.0)\n",
    "samples = gamma_dist.sample((2, 3))\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd777de7",
   "metadata": {},
   "source": [
    "You can also Padding tensor into random tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f3d727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create an uninitialized tensor, then fill it randomly.\n",
    "uninitialized = torch.empty(2, 3)\n",
    "\n",
    "# from the continuous uniform distribution\n",
    "uninitialized.uniform_()\n",
    "# rom the normal distribution parameterized\n",
    "uninitialized.normal_()\n",
    "\n",
    "# Or you can use the random methods directly\n",
    "random_tensor = torch.Tensor(2, 3).uniform_(-1, 1)\n",
    "random_tensor = torch.Tensor(2, 3).normal_(0, 1)\n",
    "random_tensor = torch.Tensor(2, 3).exponential_(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e04f6d",
   "metadata": {},
   "source": [
    "## Check tensor information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfef02ae",
   "metadata": {},
   "source": [
    "You can use the following methods to check various information about the tensor!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dae1dc7",
   "metadata": {},
   "source": [
    "### Query related dimension information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5611ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tensor = torch.randn(\n",
    "    4, 5, 6, 7, 8, dtype=torch.float16, device=device, requires_grad=True\n",
    ")\n",
    "# print(random_tensor)\n",
    "\n",
    "# shape as a list\n",
    "print(random_tensor.shape)\n",
    "print(random_tensor.shape[0])\n",
    "\n",
    "# dtype\n",
    "print(random_tensor.dtype)\n",
    "\n",
    "# device\n",
    "print(random_tensor.device)\n",
    "\n",
    "# dimensions\n",
    "print(random_tensor.ndim)\n",
    "\n",
    "# requires grad\n",
    "print(random_tensor.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce86618",
   "metadata": {},
   "source": [
    "### Query the specific content.\n",
    "\n",
    "Very similar with the `numpy` operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4472f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(random_tensor.sum())\n",
    "print(random_tensor.min())\n",
    "print(random_tensor.max())\n",
    "print(random_tensor.std())\n",
    "print(random_tensor.mean(dim=2))\n",
    "print(random_tensor.norm())\n",
    "print(random_tensor.grad)\n",
    "max_vals, max_indices = random_tensor.max(dim=1)\n",
    "print(max_indices)\n",
    "\n",
    "# Detach needs to be called if it is requires grad\n",
    "print(random_tensor.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477bcff0",
   "metadata": {},
   "source": [
    "## Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296a6c13",
   "metadata": {},
   "source": [
    "- Basic arithmetic operations and exponentiation. (Skip)\n",
    "\n",
    "- Matrix multiplication.\n",
    "\n",
    "- Linear Operations,see [this repo](https://github.com/xiyuanyang-code/numpy_tutorial) for more details. (Skip)\n",
    "\n",
    "- Autograd operations.\n",
    "\n",
    "- **Broadcasting mechanism.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e4fffd",
   "metadata": {},
   "source": [
    "### Matrix Multiplications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f110cc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplications\n",
    "\n",
    "A = torch.randn(4, 4)\n",
    "B = torch.randn(4, 4)\n",
    "\n",
    "# Element-wise multiplication of matrices.\n",
    "print(A * B)\n",
    "\n",
    "# Simple Matrix multiplications\n",
    "print(A @ B)\n",
    "\n",
    "vector_1 = torch.randn(100)\n",
    "vector_2 = torch.randn(100)\n",
    "# vector dots (for 1D only)\n",
    "print(torch.dot(vector_1, vector_2))\n",
    "\n",
    "# batch matrix multiplications\n",
    "batch_1 = torch.randn(4, 50, 10)\n",
    "batch_2 = torch.randn(4, 10, 20)\n",
    "result_batch = torch.bmm(batch_1, batch_2)\n",
    "print(result_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a356fc",
   "metadata": {},
   "source": [
    "### Autograd Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5c2293",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x**2 + 3 * x + 1\n",
    "y.backward()\n",
    "print(y.shape)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd69b81c",
   "metadata": {},
   "source": [
    "### Broadcasting mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e433a0aa",
   "metadata": {},
   "source": [
    "The same as `numpy`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f22848",
   "metadata": {},
   "source": [
    "Broadcasting follows the following rules:\n",
    "\n",
    "1. Start comparing from the trailing dimensions: Begin comparing the dimensions of tensors from the rightmost dimension and move leftward.\n",
    "\n",
    "2. Dimension compatibility conditions:\n",
    "\n",
    "   - The two dimensions are equal.\n",
    "   - One of the dimensions is 1.\n",
    "   - One of the dimensions does not exist (i.e., the tensors have different numbers of dimensions).\n",
    "   - If none of the conditions are met, an error is raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db29462",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn(1, 3)\n",
    "B = torch.randn(3, 1)\n",
    "C = torch.randn(1, 2)\n",
    "\n",
    "print(A + B)\n",
    "print(B + C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120c0235",
   "metadata": {},
   "source": [
    "## Changing Tensors' shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce01a270",
   "metadata": {},
   "source": [
    "Matrix multiplication is one of the most common operations in PyTorch, so sometimes we need to **change the shape of the created tensors**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c59f2ae",
   "metadata": {},
   "source": [
    "- `unsqueeze(dim)`: Adds a new dimension of size 1 at the specified position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7290ba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "y = x.unsqueeze(0)  # shape: (1, 3)\n",
    "z = x.unsqueeze(1)  # shape: (3, 1)\n",
    "print(y.shape)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6643c44",
   "metadata": {},
   "source": [
    "- `squeeze(dim)`: Removes dimensions of size 1. If no dim is given, all singleton dimensions are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83b3046",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(1, 3, 1, 4)\n",
    "b = a.squeeze(0)  # shape: (3, 1, 4)\n",
    "d = a.squeeze(1)  # it will do nothing\n",
    "c = a.squeeze()  # shape: (3, 4) (all size-1 dims removed)\n",
    "\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "print(c.shape)\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8563a113",
   "metadata": {},
   "source": [
    "- `reshape(new_shape)`: Returns a tensor with the same data but a new shape. May copy data if non-contiguous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f322a503",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(6)\n",
    "y = x.reshape(2, 3)  # shape: (2, 3)\n",
    "z = x.reshape(3, -1)\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b498f5",
   "metadata": {},
   "source": [
    "- `view(new_shape)`: Similar to `reshape`, but requires the tensor to be contiguous (throws an error otherwise)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8e4790",
   "metadata": {},
   "source": [
    "- `repeat_interleave(repeats, dim)`: Repeats elements of a tensor along specified dimensions (similar to NumPy’s `repeat`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50698174",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2], [3, 4]])\n",
    "y = x.repeat_interleave(2, dim=0)  # shape: (4, 2)\n",
    "# [[1, 2], [1, 2], [3, 4], [3, 4]]\n",
    "z = x.repeat_interleave(3, dim=1)  # shape: (2, 6)\n",
    "# [[1, 1, 1, 2, 2, 2], [3, 3, 3, 4, 4, 4]]\n",
    "\n",
    "print(y.shape)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c54d81",
   "metadata": {},
   "source": [
    "- `repeat`: Repeats the entire tensor along specified dimensions \n",
    "\n",
    "In PyTorch, the function `repeat()` is used to **replicate the entire tensor along specified dimensions**, thereby expanding the shape of the tensor. It differs from `repeat_interleave()`, where `repeat()` performs overall replication, while `repeat_interleave()` does element-wise replication.\n",
    "\n",
    "`repeat(*sizes)` accepts a tuple parameter that indicates the number of repetitions for each dimension and returns a new tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092419e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "print(x.shape)\n",
    "\n",
    "y = x.repeat(2)  # shape: (6,) → [1, 2, 3, 1, 2, 3]\n",
    "print(y)\n",
    "print(y.shape)\n",
    "\n",
    "z = x.repeat(3, 1)\n",
    "print(z.shape)\n",
    "print(z)\n",
    "\n",
    "w = x.repeat(2, 2)  # shape: (2, 6)\n",
    "# [[1, 2, 3, 1, 2, 3], [1, 2, 3, 1, 2, 3]]\n",
    "print(w.shape)\n",
    "print(w)\n",
    "\n",
    "test = torch.randn(3, 4, 5, 6)\n",
    "print(test.shape)\n",
    "# print(test)\n",
    "test = test.repeat(2, 2, 2, 2)\n",
    "print(test.shape)\n",
    "# print(test)\n",
    "test = test.unsqueeze(-1)\n",
    "test = test.repeat(1,1,1,1,4)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084e3e0e",
   "metadata": {},
   "source": [
    "- `expand()`: Expands a tensor to a larger size by broadcasting (only works for singleton dimensions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5243f7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1], [2], [3]])  # shape: (3, 1)\n",
    "y = x.expand(3, 4)  # shape: (3, 4)\n",
    "# [[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3]]\n",
    "print(y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
